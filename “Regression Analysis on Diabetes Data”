{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpXaQ6UwRcCv"
      },
      "source": [
        "**1)** Certainly! The least squares solution for linear regression has a unique solution when the feature matrix \\( X \\) has full column rank, meaning all columns in \\( X \\) are linearly independent. This ensures that the matrix \\( X^TX \\) is invertible. The absence of perfect multicollinearity, where no predictor is a perfect linear combination of others, is essential for \\( X \\) to maintain full column rank. If \\( X \\) does not meet these conditions, \\( X^TX \\) may not be invertible, leading to non-unique or undefined solutions. Techniques like Ridge or Lasso regression can help overcome this by modifying the loss function to ensure invertibility.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**2)** A reliable method for estimating a model's parameters when dealing with datasets that contain a large percentage of outliers is called RANSAC (Random Sample Consensus). Its power comes from its capacity to fit models to data sets tainted by outliers, a problem that classic least squares approaches are unable to solve.\n",
        "\n",
        "RANSAC is a reliable method because it can manage outliers in data sets well:\n",
        "\n",
        "Selective Sampling: It chooses random subsets of data for model fitting on a regular basis, with an emphasis on likely inliers as opposed to outliers.\n",
        "\n",
        "Validation Against Whole Set: The method counts the number of inliers in each model by comparing it to the complete dataset. The model with the highest count is then chosen.\n",
        "\n",
        "Adaptability: RANSAC may be used in a variety of settings since it doesn't require previous knowledge about the fraction of outliers.\n",
        "\n",
        "Iterative Refinement: To increase the accuracy of the final model parameters, promising models are tuned utilizing all found inliers.\n",
        "\n",
        "\n",
        "RANSAC is perfect for applications in robotics, computer vision, and any other discipline where data integrity cannot be guaranteed because of its universal applicability and robustness. It is a recommended approach for robust fitting because it can produce trustworthy models from tainted data.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**3)** The gradient descent algorithm's convergence to the loss function's minimum is significantly influenced by the learning rate. This is a succinct synopsis:\n",
        "\n",
        "High Learning Rate: When a model learns too quickly, it may overshoot the minimum and diverge or oscillate around the goal value, which can lead to unstable and ineffective training.\n",
        "Low Learning Rate: Low learning rates can cause very sluggish convergence, which raises the time and expense of computing. Moreover, there's a chance that the model will become trapped in nearby minima or plateaus, preventing it from arriving to the best answer.\n",
        "\n",
        "The optimum learning rate is one that guarantees rapid and effective convergence without overshooting, however determining this rate usually takes some trial and error.\n",
        "\n",
        "Adaptive Methods: Techniques such as AdaGrad, RMSprop, and Adam adapt the learning rate dynamically during training, enhancing the stability and efficiency of convergence by taking into account the unique properties of the data.\n",
        "\n",
        "Appropriately choosing or modifying the learning rate is crucial for efficient gradient descent optimization, impacting learning success and speed.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**4)** For a number of strong reasons, it is typically not appropriate to use least squares regression for binary classification. The main purpose of least squares is to predict continuous values rather than the binary outputs that are typically seen in classification problems. This means that the predictions produced by least squares frequently don't easily convert to binary outcomes, such as 0 or 1. Additionally, this technique performs poorly on more normal data sets due to its high sensitivity to outliers, which can disproportionately impact the model predictions as a whole. Furthermore, there is a conflict between the goal of decreasing squared errors and the primary purpose of classification, which is to reduce misclassification errors. Less squares does not offer probabilistic interpretations for class memberships, which is a critical component of successful classification, unlike logistic regression.Alternative methods created especially for classification, such logistic regression, support vector machines, and decision trees, are more appropriate in light of these disadvantages. These approaches are significantly more efficient for binary classification jobs because they are more suited to handle binary data, are less susceptible to outliers, and concentrate solely on classification accuracy.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**5)** Because L1 regularization has intrinsic qualities that encourage sparse solutions, it is sometimes referred to as Lasso and is highly respected for feature selection in model development. It successfully influences feature selection in the following ways:\n",
        "\n",
        "Sparsity Induction: A penalty proportionate to the absolute value of the coefficients is added to the loss function via L1 regularization. This penalty incentivizes the model to minimize the sum of the absolute values of the coefficients in addition to fitting the data. This has the main impact of essentially eliminating less significant characteristics from the model by driving their coefficients to zero.\n",
        "\n",
        "Automatic Feature Selection is made easier by L1 regularization's capacity to get coefficients down to zero. This helps to select and retain just those characteristics that significantly affect the response variable, which simplifies the model and may improve its interpretability. It is especially helpful for big feature models.\n",
        "\n",
        "Managing Multicollinearity: When there is a significant degree of correlation between characteristics, L1 regularization is advantageous. Unlike L2 regularization, which tends to spread coefficients across correlated features without zeroing any, it can deliberately decrease some of these associated characteristics to zero while retaining others.\n",
        "\n",
        "Enhancing Model Generalization: L1 regularization minimizes the complexity of the model, which lowers the chance of overfitting, by removing characteristics that are unnecessary or less significant. As a result, the model performs better on fresh, untested data, improving its capacity for generalization.\n",
        "\n",
        "Improved Interpretability: Less feature-heavy models are simpler to comprehend and apply. By lowering the feature count, L1 regularization aids in the creation of more straightforward models that are easier for stakeholders to understand and verify.\n",
        "\n",
        "Essentially, L1 regularization functions as a strong feature selection technique that encourages model efficacy and simplicity by concentrating on key features, boosting interpretability, and augmenting the model's capacity to generalize from training to unknown data.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        },
        "id": "zoQlFadCskgZ",
        "outputId": "9f38c6fb-f363-4383-db14-eb4c215c1076"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'numpy'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_diabetes\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "diabetes_dataset = load_diabetes()\n",
        "single_feature = diabetes_dataset.data[:, 0]\n",
        "response_variable = diabetes_dataset.target\n",
        "\n",
        "train_data = single_feature[:40].reshape(-1, 1)\n",
        "train_labels = response_variable[:40]\n",
        "test_data = single_feature[-40:].reshape(-1, 1)\n",
        "test_labels = response_variable[-40:]\n",
        "\n",
        "def compute_parameters(X, y):\n",
        "    augmented_X = np.c_[np.ones((X.shape[0], 1)), X]\n",
        "    params = np.linalg.inv(augmented_X.T @ augmented_X) @ augmented_X.T @ y\n",
        "    return params\n",
        "\n",
        "def predict_values(X, params):\n",
        "    augmented_X = np.c_[np.ones((X.shape[0], 1)), X]\n",
        "    return augmented_X @ params\n",
        "\n",
        "def plot_results(X, y, predicted, title):\n",
        "    plt.scatter(X, y, color='black', label='Actual Data')\n",
        "    plt.plot(X, predicted, color='blue', label='Model Prediction')\n",
        "    plt.xlabel('Input Feature')\n",
        "    plt.ylabel('Response Variable')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "\n",
        "ols_parameters = compute_parameters(train_data, train_labels)\n",
        "ols_predictions = predict_values(test_data, ols_parameters)\n",
        "ols_error = mean_squared_error(test_labels, ols_predictions)\n",
        "\n",
        "from sklearn.linear_model import Lasso\n",
        "lasso_regressor = Lasso(alpha=0.1)\n",
        "lasso_regressor.fit(train_data, train_labels)\n",
        "predictions_lasso = lasso_regressor.predict(test_data)\n",
        "lasso_error = mean_squared_error(test_labels, predictions_lasso)\n",
        "\n",
        "from sklearn.linear_model import Ridge\n",
        "ridge_regressor = Ridge(alpha=0.1)\n",
        "ridge_regressor.fit(train_data, train_labels)\n",
        "predictions_ridge = ridge_regressor.predict(test_data)\n",
        "ridge_error = mean_squared_error(test_labels, predictions_ridge)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 3, 1)\n",
        "plot_results(test_data.flatten(), test_labels, ols_predictions, 'OLS Regression Analysis')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plot_results(test_data.flatten(), test_labels, predictions_lasso, 'Lasso Regression Analysis')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plot_results(test_data.flatten(), test_labels, predictions_ridge, 'Ridge Regression Analysis')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "alpha_range = np.logspace(-2, 2, 100)\n",
        "coefficients = []\n",
        "for alpha in alpha_range:\n",
        "    dynamic_ridge = Ridge(alpha=alpha)\n",
        "    dynamic_ridge.fit(train_data, train_labels)\n",
        "    coefficients.append(dynamic_ridge.coef_[0])\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.semilogx(alpha_range, coefficients)\n",
        "plt.xlabel('Regularization Strength (Alpha)')\n",
        "plt.ylabel('Coefficient Magnitude')\n",
        "plt.title('Coefficient Trends in Ridge Regression')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"OLS Parameters:\", ols_parameters)\n",
        "print(\"OLS Mean Squared Error:\", ols_error)\n",
        "print(\"Lasso Coefficients:\", lasso_regressor.coef_)\n",
        "print(\"Lasso Mean Squared Error:\", lasso_error)\n",
        "print(\"Ridge Coefficients:\", ridge_regressor.coef_)\n",
        "print(\"Ridge Mean Squared Error:\", ridge_error)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
